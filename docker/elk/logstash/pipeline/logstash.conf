# Logstash Pipeline Configuration
input {
  beats {
    port => 5044
  }
  
  tcp {
    port => 5000
    codec => json
  }
}

filter {
  # Parse JSON logs from Python application
  if [message] =~ /^\{/ {
    json {
      source => "message"
      target => "parsed_json"
    }
  }
  
  # Add timestamp
  date {
    match => ["timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss"]
    target => "@timestamp"
  }
  
  # Parse log levels
  if [level] {
    mutate {
      add_field => { "log_level" => "%{level}" }
    }
  }
  
  # Extract service name from container
  if [container][name] {
    mutate {
      add_field => { "service" => "%{[container][name]}" }
    }
  }
  
  # Grok pattern for Python logs
  grok {
    match => { 
      "message" => "%{TIMESTAMP_ISO8601:timestamp} - %{LOGLEVEL:level} - %{DATA:logger} - %{GREEDYDATA:log_message}"
    }
    overwrite => ["message"]
  }
  
  # Add research system specific fields
  if [parsed_json][researcher_id] {
    mutate {
      add_field => { "researcher_id" => "%{[parsed_json][researcher_id]}" }
    }
  }
  
  # Error tracking
  if [level] == "ERROR" or [level] == "CRITICAL" {
    mutate {
      add_tag => ["error", "alert"]
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "research-logs-%{+YYYY.MM.dd}"
  }
  
  # Output errors to separate index for alerting
  if "error" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "research-errors-%{+YYYY.MM.dd}"
    }
  }
  
  # Debug output (disable in production)
  # stdout { codec => rubydebug }
}
